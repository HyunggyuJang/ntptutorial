{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural next-step prediction | part 2: learning\n",
    "Tutorial on neural theorem proving\\\n",
    "Author: Sean Welleck\n",
    "\n",
    "----------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High-level goal\n",
    "\n",
    "Our goal is to train a neural next-step predictor $p_\\theta(y_t|x_t)$ on the dataset that we collected in the previous notebook.\n",
    "\n",
    "To do so, we will fine-tune a pretrained language model on the dataset $\\mathcal{D}=\\{(x_t,y_t)\\}$ using the standard supervised fine-tuning approach:\n",
    "\n",
    "$$\n",
    "\\min_\\theta \\sum_{(x_t,y_t)\\in \\mathcal{D}}-\\log p_\\theta(y_t|x_t).\n",
    "$$\n",
    "\n",
    "That is, we maximize the conditional likelihood of a next-step $y_t$ given the context $x_t$. \\\n",
    "This corresponds to minimizing a cross-entropy loss at each position of the next-step, $\\sum_{\\ell=1}^{{|y_t|}}-\\log p_\\theta(y_t^\\ell|y_t^{<\\ell})$.\n",
    "\n",
    "This is because that we can think $x_t$ as the state after applying $y_t^{<\\ell}$ to $x_1$. So, the former formulation also includes the later formulation, with one additional info, the initial state, $x_1$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "The implementation consists of two steps:\n",
    "\n",
    "1. **Data formatting** ([data.py](../ntp_python/data.py)): formatting the examples.\n",
    "2. **Tuning**  ([tune.py](../ntp_python/tune.py)): using a standard language model fine-tuning script.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data formatting\n",
    "\n",
    "We format each (tactic-state, next-step) pair $(x_t, y_t)$ as:\n",
    "\n",
    "        [GOAL]tacticstate[PROOFSTEP]next-step<|endoftext|>\n",
    "\n",
    "Here, `[GOAL]...[PROOFSTEP]` is the input and `next-step<|endoftext|>` is the output.\n",
    "\n",
    "This format comes from [Han et al ICLR 2022]: \\\n",
    "[Proof Artifact Co-training for Theorem Proving with Language Models](https://arxiv.org/pdf/2102.06203.pdf).\n",
    "\n",
    "<!-- *Exercise:* can you think of other auxiliary tasks that might be useful? -->\n",
    "\n",
    "<!-- *Exercise:* can you think of alternative formats, e.g. which provide additional context? -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving split to disk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\t169530\n",
      "val\t4053\n",
      "test\t3606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../ntp_python')\n",
    "import data\n",
    "\n",
    "datasets = data.proofstep(\n",
    "    data_dir='../data'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "[GOAL]ι : Type u_1\n",
      "I✝ J✝ : Box ι\n",
      "x y : ι → ℝ\n",
      "I J : WithBot (Box ι)\n",
      "⊢ ↑I = ↑J ↔ I = J[PROOFSTEP]\n",
      "\n",
      "Output:\n",
      "simp only [Subset.antisymm_iff, ← le_antisymm_iff, withBotCoe_subset_iff]<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "example = datasets['train'][0]\n",
    "print(\"Input:\", example['input'], '', sep='\\n')\n",
    "print(\"Output:\", example['output'], sep='\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Tuning\n",
    "\n",
    "We minimally adapt a standard language-model fine-tuning script from [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca/blob/main/train.py). \n",
    "\n",
    "You can check out the full script at [partI_nextstep/ntp_python/tune.py](../ntp_python/tune.py). \\\n",
    "See [partI_nextstep/scripts/tune_proofstep.sh](../scripts/tune_proofstep.sh) for a command that trains on 8 GPUs with deepspeed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example command for training a 1.4b model on 1 GPU (and you can adjust the model size to be smaller to fit your compute constraints):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "REPO_DIR=\"..\"\n",
    "TRAIN_FILE=${REPO_DIR}/data/processed/proofstep-train.jsonl\n",
    "VALID_FILE=${REPO_DIR}/data/processed/proofstep-val.jsonl\n",
    "MODEL=EleutherAI/pythia-1.4b-deduped\n",
    "\n",
    "OUTDIR=${REPO_DIR}/model/${MODEL}\n",
    "\n",
    "python ../ntp_python/tune.py \\\n",
    "    --model_name_or_path ${MODEL} \\\n",
    "    --train_data_path ${TRAIN_FILE} \\\n",
    "    --valid_data_path ${VALID_FILE} \\\n",
    "    --fp16 \\\n",
    "    --output_dir ${OUTDIR} \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --learning_rate 1e-5 \\\n",
    "    --weight_decay 0. \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 10 \\\n",
    "    --logging_dir \"$OUTDIR\" \\\n",
    "    --report_to=\"tensorboard\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, let's train it via real script. Since the [partI_nextstep/scripts/tune_proofstep.sh](../scripts/tune_proofstep.sh) was partly obsolete, so I amended a bit. Let's train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct  8 08:25:17 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
      "| N/A   34C    P8              11W /  72W |      4MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA L4                      Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   34C    P8              12W /  72W |      4MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA L4                      Off | 00000000:00:05.0 Off |                    0 |\n",
      "| N/A   33C    P8              12W /  72W |      4MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA L4                      Off | 00000000:00:06.0 Off |                    0 |\n",
      "| N/A   34C    P8              13W /  72W |      4MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA L4                      Off | 00000000:80:00.0 Off |                    0 |\n",
      "| N/A   34C    P8              11W /  72W |      4MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA L4                      Off | 00000000:80:01.0 Off |                    0 |\n",
      "| N/A   35C    P8              11W /  72W |      4MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA L4                      Off | 00000000:80:02.0 Off |                    0 |\n",
      "| N/A   36C    P8              12W /  72W |      4MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA L4                      Off | 00000000:80:03.0 Off |                    0 |\n",
      "| N/A   34C    P8              11W /  72W |      4MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.6\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python version seems safe to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting ndjson\n",
      "  Downloading ndjson-0.3.1-py2.py3-none-any.whl (5.3 kB)\n",
      "Installing collected packages: ndjson\n",
      "Successfully installed ndjson-0.3.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ndjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-08 08:25:43,068] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-10-08 08:25:56,331] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2023-10-08 08:25:56,331] [INFO] [runner.py:570:main] cmd = /usr/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None ../ntp_python/tune.py --deepspeed ../scripts/ds_config.json --model_name_or_path EleutherAI/pythia-2.8b-deduped --train_data_path ../data/processed/proofstep-train.jsonl --valid_data_path ../data/processed/proofstep-val.jsonl --fp16 --output_dir ../model/EleutherAI/pythia-2.8b-deduped --num_train_epochs 10 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy steps --eval_steps 500 --save_strategy steps --save_steps 500 --save_total_limit 1 --learning_rate 1e-5 --load_best_model_at_end 1 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 10 --logging_dir ../model/EleutherAI/pythia-2.8b-deduped --report_to=tensorboard\n",
      "[2023-10-08 08:26:00,870] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-10-08 08:26:11,305] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.18.3\n",
      "[2023-10-08 08:26:11,305] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\n",
      "[2023-10-08 08:26:11,306] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0\n",
      "[2023-10-08 08:26:11,306] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\n",
      "[2023-10-08 08:26:11,306] [INFO] [launch.py:163:main] dist_world_size=8\n",
      "[2023-10-08 08:26:11,306] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "[2023-10-08 08:26:33,167] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-10-08 08:26:33,234] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-10-08 08:26:33,252] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-10-08 08:26:33,261] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-10-08 08:26:33,286] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-10-08 08:26:33,322] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-10-08 08:26:33,996] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-10-08 08:26:34,302] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-10-08 08:26:34,978] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-10-08 08:26:34,978] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-10-08 08:26:34,978] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-10-08 08:26:34,978] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-10-08 08:26:34,978] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-10-08 08:26:34,979] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2023-10-08 08:26:34,979] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-10-08 08:26:35,622] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "Downloading (…)lve/main/config.json: 100%|█████| 571/571 [00:00<00:00, 2.51MB/s]\n",
      "[2023-10-08 08:26:35,936] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "Downloading model.safetensors: 100%|███████| 5.68G/5.68G [02:07<00:00, 44.5MB/s]\n",
      "[2023-10-08 08:28:44,268] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\n",
      "[2023-10-08 08:28:44,269] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\n",
      "[2023-10-08 08:28:44,269] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\n",
      "[2023-10-08 08:28:44,269] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\n",
      "[2023-10-08 08:28:44,269] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\n",
      "[2023-10-08 08:28:44,273] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\n",
      "[2023-10-08 08:28:44,290] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\n",
      "[2023-10-08 08:28:44,291] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter stage3_gather_fp16_weights_on_model_save is deprecated use gather_16bit_weights_on_model_save instead\n",
      "[5d904d038f1b:17787:0:18995] Caught signal 7 (Bus error: nonexistent physical address)\n",
      "[5d904d038f1b:17786:0:18997] Caught signal 7 (Bus error: nonexistent physical address)\n",
      "[5d904d038f1b:17783:0:18999] Caught signal 7 (Bus error: nonexistent physical address)\n",
      "[5d904d038f1b:17781:0:18996] Caught signal 7 (Bus error: nonexistent physical address)\n",
      "[5d904d038f1b:17782:0:19000] Caught signal 7 (Bus error: nonexistent physical address)\n",
      "[5d904d038f1b:17784:0:18993] Caught signal 7 (Bus error: nonexistent physical address)\n",
      "[5d904d038f1b:17788:0:18994] Caught signal 7 (Bus error: nonexistent physical address)\n",
      "[5d904d038f1b:17785:0:18998] Caught signal 7 (Bus error: nonexistent physical address)\n",
      "==== backtrace (tid:  18999) ====\n",
      "==== backtrace (tid:  18996) ====\n",
      " 0 0x0000000000042520 __sigaction()  ???:0\n",
      " 0 0x0000000000042520 __sigaction()  ???:0\n",
      " 1 0x00000000001afbba __nss_database_lookup()  ???:0\n",
      " 1 0x00000000001afbba __nss_database_lookup()  ???:0\n",
      " 2 0x000000000008e445 ncclGroupEnd()  ???:0\n",
      " 2 0x000000000008e445 ncclGroupEnd()  ???:0\n",
      " 3 0x0000000000094f1c ncclGroupEnd()  ???:0\n",
      " 3 0x0000000000094f1c ncclGroupEnd()  ???:0\n",
      " 4 0x000000000006e41c ncclCommAbort()  ???:0\n",
      " 4 0x000000000006e41c ncclCommAbort()  ???:0\n",
      " 5 0x000000000005ca77 ncclCommAbort()  ???:0\n",
      " 5 0x000000000005ca77 ncclCommAbort()  ???:0\n",
      " 6 0x000000000005fe94 ncclCommAbort()  ???:0\n",
      " 6 0x000000000005fe94 ncclCommAbort()  ???:0\n",
      " 7 0x000000000007912c pncclRedOpDestroy()  ???:0\n",
      " 7 0x000000000007912c pncclRedOpDestroy()  ???:0\n",
      " 8 0x0000000000094b43 pthread_condattr_setpshared()  ???:0\n",
      " 8 0x0000000000094b43 pthread_condattr_setpshared()  ???:0\n",
      " 9 0x0000000000125bb4 clone()  ???:0\n",
      " 9 0x0000000000125bb4 clone()  ???:0\n",
      "=================================\n",
      "=================================\n",
      "==== backtrace (tid:  19000) ====\n",
      " 0 0x0000000000042520 __sigaction()  ???:0\n",
      " 1 0x00000000001afbba __nss_database_lookup()  ???:0\n",
      " 2 0x000000000008e445 ncclGroupEnd()  ???:0\n",
      " 3 0x0000000000094f1c ncclGroupEnd()  ???:0\n",
      " 4 0x000000000006e41c ncclCommAbort()  ???:0\n",
      " 5 0x000000000005ca77 ncclCommAbort()  ???:0\n",
      " 6 0x000000000005fe94 ncclCommAbort()  ???:0\n",
      " 7 0x000000000007912c pncclRedOpDestroy()  ???:0\n",
      " 8 0x0000000000094b43 pthread_condattr_setpshared()  ???:0\n",
      " 9 0x0000000000125bb4 clone()  ???:0\n",
      "=================================\n",
      "==== backtrace (tid:  18997) ====\n",
      " 0 0x0000000000042520 __sigaction()  ???:0\n",
      " 1 0x00000000001afbba __nss_database_lookup()  ???:0\n",
      " 2 0x000000000008e445 ncclGroupEnd()  ???:0\n",
      " 3 0x0000000000094f1c ncclGroupEnd()  ???:0\n",
      " 4 0x000000000006e41c ncclCommAbort()  ???:0\n",
      " 5 0x000000000005ca77 ncclCommAbort()  ???:0\n",
      " 6 0x000000000005fe94 ncclCommAbort()  ???:0\n",
      " 7 0x000000000007912c pncclRedOpDestroy()  ???:0\n",
      " 8 0x0000000000094b43 pthread_condattr_setpshared()  ???:0\n",
      " 9 0x0000000000125bb4 clone()  ???:0\n",
      "=================================\n",
      "==== backtrace (tid:  18993) ====\n",
      " 0 0x0000000000042520 __sigaction()  ???:0\n",
      " 1 0x00000000001afbba __nss_database_lookup()  ???:0\n",
      " 2 0x000000000008e445 ncclGroupEnd()  ???:0\n",
      " 3 0x0000000000094f1c ncclGroupEnd()  ???:0\n",
      " 4 0x000000000006e41c ncclCommAbort()  ???:0\n",
      " 5 0x000000000005ca77 ncclCommAbort()  ???:0\n",
      " 6 0x000000000005fe94 ncclCommAbort()  ???:0\n",
      " 7 0x000000000007912c pncclRedOpDestroy()  ???:0\n",
      " 8 0x0000000000094b43 pthread_condattr_setpshared()  ???:0\n",
      " 9 0x0000000000125bb4 clone()  ???:0\n",
      "=================================\n",
      "==== backtrace (tid:  18995) ====\n",
      " 0 0x0000000000042520 __sigaction()  ???:0\n",
      " 1 0x00000000001afbba __nss_database_lookup()  ???:0\n",
      " 2 0x000000000008e445 ncclGroupEnd()  ???:0\n",
      " 3 0x0000000000094f1c ncclGroupEnd()  ???:0\n",
      " 4 0x000000000006e41c ncclCommAbort()  ???:0\n",
      " 5 0x000000000005ca77 ncclCommAbort()  ???:0\n",
      " 6 0x000000000005fe94 ncclCommAbort()  ???:0\n",
      " 7 0x000000000007912c pncclRedOpDestroy()  ???:0\n",
      " 8 0x0000000000094b43 pthread_condattr_setpshared()  ???:0\n",
      " 9 0x0000000000125bb4 clone()  ???:0\n",
      "=================================\n",
      "==== backtrace (tid:  18998) ====\n",
      " 0 0x0000000000042520 __sigaction()  ???:0\n",
      " 1 0x00000000001afbba __nss_database_lookup()  ???:0\n",
      " 2 0x000000000008e445 ncclGroupEnd()  ???:0\n",
      " 3 0x0000000000094f1c ncclGroupEnd()  ???:0\n",
      " 4 0x000000000006e41c ncclCommAbort()  ???:0\n",
      " 5 0x000000000005ca77 ncclCommAbort()  ???:0\n",
      " 6 0x000000000005fe94 ncclCommAbort()  ???:0\n",
      " 7 0x000000000007912c pncclRedOpDestroy()  ???:0\n",
      " 8 0x0000000000094b43 pthread_condattr_setpshared()  ???:0\n",
      " 9 0x0000000000125bb4 clone()  ???:0\n",
      "=================================\n",
      "==== backtrace (tid:  18994) ====\n",
      " 0 0x0000000000042520 __sigaction()  ???:0\n",
      " 1 0x00000000001afbba __nss_database_lookup()  ???:0\n",
      " 2 0x000000000008e445 ncclGroupEnd()  ???:0\n",
      " 3 0x0000000000094f1c ncclGroupEnd()  ???:0\n",
      " 4 0x000000000006e41c ncclCommAbort()  ???:0\n",
      " 5 0x000000000005ca77 ncclCommAbort()  ???:0\n",
      " 6 0x000000000005fe94 ncclCommAbort()  ???:0\n",
      " 7 0x000000000007912c pncclRedOpDestroy()  ???:0\n",
      " 8 0x0000000000094b43 pthread_condattr_setpshared()  ???:0\n",
      " 9 0x0000000000125bb4 clone()  ???:0\n",
      "=================================\n",
      "[2023-10-08 08:28:47,489] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 17781\n",
      "[2023-10-08 08:28:47,490] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 17782\n",
      "[2023-10-08 08:28:47,492] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 17783\n",
      "[2023-10-08 08:28:47,494] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 17784\n",
      "[2023-10-08 08:28:47,496] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 17785\n",
      "[2023-10-08 08:28:47,629] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 17786\n",
      "[2023-10-08 08:28:47,631] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 17787\n",
      "[2023-10-08 08:28:47,633] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 17788\n",
      "[2023-10-08 08:28:47,634] [ERROR] [launch.py:321:sigkill_handler] ['/usr/bin/python', '-u', '../ntp_python/tune.py', '--local_rank=7', '--deepspeed', '../scripts/ds_config.json', '--model_name_or_path', 'EleutherAI/pythia-2.8b-deduped', '--train_data_path', '../data/processed/proofstep-train.jsonl', '--valid_data_path', '../data/processed/proofstep-val.jsonl', '--fp16', '--output_dir', '../model/EleutherAI/pythia-2.8b-deduped', '--num_train_epochs', '10', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'steps', '--eval_steps', '500', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '1e-5', '--load_best_model_at_end', '1', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--logging_dir', '../model/EleutherAI/pythia-2.8b-deduped', '--report_to=tensorboard'] exits with return code = -7\n"
     ]
    }
   ],
   "source": [
    "!sh ../scripts/tune_proofstep.sh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After training\n",
    "\n",
    "If everything went well, you should have a model in `../model/{MODEL_NAME}/checkpoint-{BEST_STEP}`.\n",
    "\n",
    "We have fine-tuned an `EleutherAI/pythia-2.8b-deduped` model that can be accessed through HuggingFace ([link](https://huggingface.co/wellecks/llmstep-mathlib4-pythia2.8b)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "MODEL = 'wellecks/llmstep-mathlib4-pythia2.8b'\n",
    "model = transformers.GPTNeoXForCausalLM.from_pretrained(MODEL)\n",
    "tokenizer = transformers.GPTNeoXTokenizerFast.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use your own model by setting `MODEL = \"../model/{MODEL_NAME}/checkpoint-{BEST_STEP}\"` \\\n",
    "(e.g., `../model/EleutherAI/pythia-2.8b-deduped/checkpoint-5000`)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a next-step suggestion for the proof state from our original example:\n",
    "\n",
    "```lean\n",
    "    theorem test_thm (m n : Nat) (h : m.coprime n) : m.gcd n = 1\n",
    "```\n",
    "Recal from the previous notebook that the initial proof state $x_0$ is:\n",
    "\n",
    "        m n : ℕ\n",
    "        h : Nat.coprime m n\n",
    "        ⊢ Nat.gcd m n = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rw [← h.gcd_eq_one]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"[GOAL]m n : ℕ\n",
    "  h : Nat.coprime m n\n",
    "  ⊢ Nat.gcd m n = 1[PROOFSTEP]\"\"\"\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "out = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=256,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "text = tokenizer.decode(out[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "\n",
    "In the next notebook, we will prove theorems with the trained model by interacting with the Lean proof assistant.\n",
    "\n",
    "This will let us automatically check whether a generated proof (e.g., one containing the step above) is correct.\n",
    "\n",
    "Later on, we will build a VSCode plugin that returns next-step suggestions from the language model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
